{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.providers.http.operators.http import SimpleHttpOperator\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from cosmos import DbtTaskGroup\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Função para processar dados extraídos da API\n",
    "def process_api_data(**kwargs):\n",
    "    ti = kwargs['ti']\n",
    "    print(\"[LOG] Iniciando processamento dos dados da API...\")\n",
    "    api_data = ti.xcom_pull(task_ids='extract_api_data')\n",
    "    print(f\"[LOG] Dados extraídos da API: {api_data}\")\n",
    "    df = pd.DataFrame(api_data['results'])\n",
    "    print(f\"[LOG] DataFrame criado com {len(df)} linhas\")\n",
    "    df.to_csv('/path/to/data/api_data.csv', index=False)\n",
    "    print(\"[LOG] Dados da API salvos em /path/to/data/api_data.csv\")\n",
    "\n",
    "# DAG principal\n",
    "with DAG(\n",
    "    'cosmos_multi_source_etl',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False\n",
    ") as dag:\n",
    "\n",
    "    # Etapa 1: Extrai dados de uma API\n",
    "    print(\"[LOG] Configurando extração da API...\")\n",
    "    extract_api_data = SimpleHttpOperator(\n",
    "        task_id='extract_api_data',\n",
    "        method='GET',\n",
    "        http_conn_id='api_conn',\n",
    "        endpoint='data_endpoint',\n",
    "        do_xcom_push=True\n",
    "    )\n",
    "\n",
    "    # Etapa 2: Extrai dados de um banco de dados SQL\n",
    "    print(\"[LOG] Configurando extração do banco SQL...\")\n",
    "    extract_sql_data = PostgresOperator(\n",
    "        task_id='extract_sql_data',\n",
    "        postgres_conn_id='postgres_conn',\n",
    "        sql=\"\"\"\n",
    "        COPY (SELECT * FROM sales_data WHERE date = CURRENT_DATE)\n",
    "        TO '/path/to/data/sql_data.csv' CSV HEADER;\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Etapa 3: Processa os dados da API\n",
    "    print(\"[LOG] Configurando processamento dos dados da API...\")\n",
    "    process_data = PythonOperator(\n",
    "        task_id='process_data',\n",
    "        python_callable=process_api_data\n",
    "    )\n",
    "\n",
    "    # Etapa 4: Usa Cosmos para rodar transformações com dbt\n",
    "    print(\"[LOG] Configurando transformações dbt com Cosmos...\")\n",
    "    dbt_transforms = DbtTaskGroup(\n",
    "        group_id='dbt_transformations',\n",
    "        dir='/path/to/dbt_project',\n",
    "        profile_args={\"target\": \"prod\"},\n",
    "    )\n",
    "\n",
    "    # Etapa 5: Carrega os dados no Data Warehouse\n",
    "    print(\"[LOG] Configurando carga de dados no Data Warehouse...\")\n",
    "    load_data = PostgresOperator(\n",
    "        task_id='load_data',\n",
    "        postgres_conn_id='warehouse_conn',\n",
    "        sql=\"\"\"\n",
    "        COPY transformed_data FROM '/path/to/dbt_project/target/output.csv' CSV HEADER;\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Define a ordem das tarefas\n",
    "    print(\"[LOG] Definindo ordem das tarefas...\")\n",
    "    [extract_api_data, extract_sql_data] >> process_data >> dbt_transforms >> load_data\n",
    "\n",
    "    print(\"[LOG] DAG configurada com sucesso!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
